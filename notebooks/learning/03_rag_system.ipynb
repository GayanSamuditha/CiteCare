{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 3: Building the RAG System\n",
    "\n",
    "In this notebook, you'll learn how to:\n",
    "1. Store embeddings in a vector database (ChromaDB)\n",
    "2. Retrieve relevant documents based on a query\n",
    "3. Build a complete RAG chain that answers questions using your documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the project root to the path\n",
    "import sys\n",
    "sys.path.insert(0, '../..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Creating a Vector Store\n",
    "\n",
    "ChromaDB is a simple, local vector database that stores embeddings and allows fast similarity search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from src.document_loader import load_and_split\n",
    "\n",
    "# Load and split documents\n",
    "chunks = load_and_split()\n",
    "\n",
    "# Initialize embeddings\n",
    "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "\n",
    "# Create vector store from documents\n",
    "# This embeds all chunks and stores them\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embeddings,\n",
    "    collection_name=\"learning_demo\"\n",
    ")\n",
    "\n",
    "print(f\"Created vector store with {len(chunks)} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Similarity Search\n",
    "\n",
    "The vector store can find documents similar to a query by comparing embedding vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for documents similar to a query\n",
    "query = \"What is supervised learning?\"\n",
    "results = vectorstore.similarity_search(query, k=3)\n",
    "\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"\\nFound {len(results)} relevant chunks:\\n\")\n",
    "\n",
    "for i, doc in enumerate(results):\n",
    "    print(f\"--- Result {i+1} ---\")\n",
    "    print(doc.page_content[:300])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search with scores to see similarity values\n",
    "results_with_scores = vectorstore.similarity_search_with_score(query, k=3)\n",
    "\n",
    "print(\"Results with similarity scores:\")\n",
    "print(\"(Lower score = more similar for ChromaDB)\\n\")\n",
    "\n",
    "for doc, score in results_with_scores:\n",
    "    print(f\"Score: {score:.4f}\")\n",
    "    print(f\"Content: {doc.page_content[:100]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Creating a Retriever\n",
    "\n",
    "A retriever is a wrapper around the vector store that's designed to work with LangChain chains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a retriever from the vector store\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "# Use the retriever\n",
    "docs = retriever.invoke(\"What are the types of machine learning?\")\n",
    "\n",
    "print(f\"Retrieved {len(docs)} documents\")\n",
    "for doc in docs:\n",
    "    print(f\"- {doc.page_content[:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Building the RAG Chain\n",
    "\n",
    "Now we combine the retriever with an LLM to create a complete RAG system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = ChatOllama(model=\"llama3.2:3b\")\n",
    "\n",
    "# Create the RAG prompt\n",
    "rag_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are a helpful assistant. Answer the question based ONLY on the following context.\n",
    "If the context doesn't contain the answer, say \"I don't have enough information.\"\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\")\n",
    "\n",
    "# Helper function to format documents\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Build the RAG chain\n",
    "rag_chain = (\n",
    "    RunnableParallel(\n",
    "        context=retriever | format_docs,\n",
    "        question=RunnablePassthrough()\n",
    "    )\n",
    "    | rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(\"RAG chain created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the RAG chain\n",
    "question = \"What is supervised learning and what are some examples?\"\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "answer = rag_chain.invoke(question)\n",
    "print(f\"Answer:\\n{answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try more questions\n",
    "questions = [\n",
    "    \"What is reinforcement learning used for?\",\n",
    "    \"What is overfitting?\",\n",
    "    \"What is the capital of France?\"  # Not in our documents!\n",
    "]\n",
    "\n",
    "for q in questions:\n",
    "    print(f\"Q: {q}\")\n",
    "    print(f\"A: {rag_chain.invoke(q)}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Using Our Project Modules\n",
    "\n",
    "Our `src/` modules wrap all this functionality for easy reuse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.vectorstore import get_or_create_vectorstore\n",
    "from src.rag_chain import create_rag_chain, query_rag\n",
    "\n",
    "# Get or create the vector store (uses default persist location)\n",
    "vs = get_or_create_vectorstore(chunks)\n",
    "\n",
    "# Create RAG chain\n",
    "chain = create_rag_chain(vs)\n",
    "\n",
    "# Query using our helper function\n",
    "answer = query_rag(chain, \"What are the three types of machine learning?\", verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Vector Stores** (ChromaDB) store embeddings for fast similarity search\n",
    "2. **Similarity Search** finds documents semantically similar to a query\n",
    "3. **Retrievers** are chain-compatible wrappers around vector stores\n",
    "4. **RAG Chain** = Retriever → Format Context → Prompt → LLM → Parser\n",
    "5. The LLM answers are **grounded** in the retrieved documents\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "In the final phase, you'll add:\n",
    "- A CLI interface for easy interaction\n",
    "- Conversation memory for follow-up questions\n",
    "- Source citations to show which documents were used"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
