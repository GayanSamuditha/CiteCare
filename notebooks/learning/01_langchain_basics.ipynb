{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1: LangChain Basics\n",
    "\n",
    "In this notebook, you'll learn the fundamentals of LangChain:\n",
    "1. Connecting to local LLMs via Ollama\n",
    "2. Using Prompt Templates\n",
    "3. Building Chains with LCEL (LangChain Expression Language)\n",
    "\n",
    "## Prerequisites\n",
    "- Ollama running with `llama3.2:3b` model\n",
    "- Virtual environment activated with dependencies installed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Connecting to Ollama\n",
    "\n",
    "LangChain provides the `ChatOllama` class to connect to local models running via Ollama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# Initialize the chat model\n",
    "# This connects to Ollama running locally on port 11434 (default)\n",
    "llm = ChatOllama(model=\"llama3.2:3b\")\n",
    "\n",
    "# Test the connection with a simple query\n",
    "response = llm.invoke(\"Hello! What is 2 + 2?\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Understanding Prompt Templates\n",
    "\n",
    "Prompt templates let you create reusable prompts with placeholders for variables.\n",
    "This is essential for building flexible applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Create a simple prompt template with a variable {topic}\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Explain {topic} in simple terms that a beginner would understand.\"\n",
    ")\n",
    "\n",
    "# Preview what the prompt looks like when filled in\n",
    "formatted = prompt.format(topic=\"photosynthesis\")\n",
    "print(\"Formatted prompt:\")\n",
    "print(formatted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Building Your First Chain (LCEL)\n",
    "\n",
    "LCEL (LangChain Expression Language) uses the `|` operator to chain components together.\n",
    "Data flows from left to right: `input -> prompt -> llm -> output`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a chain by connecting prompt to llm with the pipe operator\n",
    "chain = prompt | llm\n",
    "\n",
    "# Invoke the chain with input variables\n",
    "response = chain.invoke({\"topic\": \"photosynthesis\"})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Adding an Output Parser\n",
    "\n",
    "Output parsers transform the LLM response into a more usable format.\n",
    "The `StrOutputParser` extracts just the text content from the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Add output parser to get clean string output\n",
    "chain_with_parser = prompt | llm | StrOutputParser()\n",
    "\n",
    "# Now the response is a plain string, not an AIMessage object\n",
    "response = chain_with_parser.invoke({\"topic\": \"machine learning\"})\n",
    "print(type(response))  # Should be <class 'str'>\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Multi-Variable Prompts\n",
    "\n",
    "Prompts can have multiple variables for more complex use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prompt with multiple variables\n",
    "multi_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"You are a {role}. \n",
    "    \n",
    "Please explain {topic} to someone who is a {audience}.\n",
    "Keep your explanation {length}.\"\"\"\n",
    ")\n",
    "\n",
    "# Create the chain\n",
    "multi_chain = multi_prompt | llm | StrOutputParser()\n",
    "\n",
    "# Invoke with all variables\n",
    "response = multi_chain.invoke({\n",
    "    \"role\": \"friendly science teacher\",\n",
    "    \"topic\": \"DNA\",\n",
    "    \"audience\": \"10-year-old student\",\n",
    "    \"length\": \"brief (2-3 sentences)\"\n",
    "})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. System Messages and Chat History\n",
    "\n",
    "For chat models, you can use system messages to set the AI's behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "\n",
    "# Create a prompt with system message and message history\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant who speaks like a pirate.\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "chat_chain = chat_prompt | llm | StrOutputParser()\n",
    "\n",
    "response = chat_chain.invoke({\"question\": \"What's the weather like?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **ChatOllama** connects LangChain to local Ollama models\n",
    "2. **ChatPromptTemplate** creates reusable prompts with variables\n",
    "3. **LCEL (|)** chains components together: `prompt | llm | parser`\n",
    "4. **StrOutputParser** converts AI responses to plain strings\n",
    "5. System messages control the AI's personality and behavior\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "In the next notebook, you'll learn about:\n",
    "- Loading documents from files\n",
    "- Splitting text into chunks\n",
    "- Creating embeddings for semantic search"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
