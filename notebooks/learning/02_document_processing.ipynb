{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2: Document Processing\n",
    "\n",
    "In this notebook, you'll learn how to:\n",
    "1. Load documents from files\n",
    "2. Split documents into chunks\n",
    "3. Generate embeddings for semantic search\n",
    "\n",
    "These are the foundational steps for building a RAG system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the project root to the path so we can import our modules\n",
    "import sys\n",
    "sys.path.insert(0, '../..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading Documents\n",
    "\n",
    "LangChain provides various document loaders for different file types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "# Load a text file\n",
    "loader = TextLoader('../../data/documents/sample_article.txt', encoding='utf-8')\n",
    "documents = loader.load()\n",
    "\n",
    "print(f\"Loaded {len(documents)} document(s)\")\n",
    "print(f\"\\nDocument content preview (first 500 chars):\")\n",
    "print(documents[0].page_content[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the document metadata\n",
    "print(\"Document metadata:\")\n",
    "print(documents[0].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Splitting Documents into Chunks\n",
    "\n",
    "Documents are typically too long to embed directly. We split them into smaller chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Create a text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,       # Maximum characters per chunk\n",
    "    chunk_overlap=50,     # Overlap between chunks for context\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]  # Priority order for splitting\n",
    ")\n",
    "\n",
    "# Split the documents\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"Split into {len(chunks)} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine a few chunks\n",
    "for i, chunk in enumerate(chunks[:3]):\n",
    "    print(f\"\\n--- Chunk {i+1} ({len(chunk.page_content)} chars) ---\")\n",
    "    print(chunk.page_content[:200] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Understanding Embeddings\n",
    "\n",
    "Embeddings convert text into numerical vectors that capture semantic meaning.\n",
    "Similar texts will have similar embedding vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "# Initialize the embeddings model\n",
    "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "\n",
    "# Embed a single text\n",
    "test_text = \"Machine learning is a type of artificial intelligence.\"\n",
    "vector = embeddings.embed_query(test_text)\n",
    "\n",
    "print(f\"Text: {test_text}\")\n",
    "print(f\"Embedding dimension: {len(vector)}\")\n",
    "print(f\"First 10 values: {vector[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Semantic Similarity\n",
    "\n",
    "Let's see how embeddings capture semantic similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    \"\"\"Calculate cosine similarity between two vectors.\"\"\"\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "\n",
    "# Test semantic similarity\n",
    "sentences = [\n",
    "    \"Machine learning helps computers learn from data.\",\n",
    "    \"AI systems can improve through experience and data.\",  # Similar meaning\n",
    "    \"The weather today is sunny and warm.\"  # Different topic\n",
    "]\n",
    "\n",
    "# Embed all sentences\n",
    "vectors = embeddings.embed_documents(sentences)\n",
    "\n",
    "# Compare similarities to the first sentence\n",
    "print(f\"Base sentence: '{sentences[0]}'\\n\")\n",
    "for i in range(1, len(sentences)):\n",
    "    similarity = cosine_similarity(vectors[0], vectors[i])\n",
    "    print(f\"Similarity to '{sentences[i][:50]}...': {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Using Our Project Modules\n",
    "\n",
    "Now let's use the modules we created in the `src/` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.document_loader import load_and_split\n",
    "from src.embeddings import get_embeddings, embed_text\n",
    "\n",
    "# Load and split all documents from the data directory\n",
    "chunks = load_and_split()\n",
    "\n",
    "print(f\"\\nTotal chunks ready for embedding: {len(chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test embedding a chunk\n",
    "if chunks:\n",
    "    sample_chunk = chunks[0].page_content\n",
    "    vector = embed_text(sample_chunk)\n",
    "    print(f\"Embedded chunk with {len(vector)} dimensions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Document Loaders** read files into LangChain Document objects\n",
    "2. **Text Splitters** break large documents into manageable chunks\n",
    "3. **RecursiveCharacterTextSplitter** tries to preserve semantic boundaries\n",
    "4. **Embeddings** convert text to numerical vectors for similarity search\n",
    "5. **Cosine Similarity** measures how similar two embedding vectors are\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "In the next notebook, you'll learn how to:\n",
    "- Store embeddings in a vector database (ChromaDB)\n",
    "- Retrieve relevant documents based on a query\n",
    "- Build a complete RAG chain"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
